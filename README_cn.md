# README

这是一个fork自[karpathy/llm.c](https://github.com/karpathy/llm.c)的个人小项目, 有这个项目的起因是自己从事模型训练方面的工作, 一直以来的工作方向都偏向于大型离散模型, 对数据并行方面较熟悉, 而对模型并行和现在大模型训练最新的优化技术只是通过看别人写的博客进行了解, 然而看完就忘, 所谓一知半解大概就是自己目前所处的阶段吧.

古人云: 纸上得来终觉浅，绝知此事要躬行. 希望通过手写GPT2的项目, 对下面的技术加深理解. 同时也作为记录自己学习LLM和总结过往工作经验的一个小博客, 加油!

-   FlashAttention(TODO)
-   ZeRO(from DeepSpeed TODO)
-   Template MetaProgramming (TODO)
-   CUDA kernel optimize(TODO)

